{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined keywords = 'Gender','sex',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred: HTTPSConnectionPool(host='go.boarddocs.com', port=443): Max retries exceeded with url: /pa/bren/Board.nsf/goto?open&id=9X4P6A630F4D (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000170B19B6B80>: Failed to resolve 'go.boarddocs.com' ([Errno 11001] getaddrinfo failed)\")). Saving current data to CSV...\n",
      "Data saved. Script crashed.\n",
      "Exception occurred: HTTPSConnectionPool(host='go.boarddocs.com', port=443): Max retries exceeded with url: /pa/bren/Board.nsf/goto?open&id=AK2SXY74E3B0 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000170B1989490>: Failed to resolve 'go.boarddocs.com' ([Errno 11001] getaddrinfo failed)\")). Saving current data to CSV...\n",
      "Data saved. Script crashed.\n",
      "Container div not found for https://go.boarddocs.com/pa/carm/Board.nsf/goto?open&id=A8LV646A2E56\n",
      "Extracted data saved to policy1_extracted_data_2023-08-15.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Read in the main database\n",
    "a1 = pd.read_csv(\"school_mainPA.csv\")\n",
    "a1 = a1[0:25]\n",
    "\n",
    "# keywords = [\"sexuality\",'at birth','male puberty','Biological','Critical race theory',\n",
    "#             'Partisan','Political','Indoctrination','Advocacy','Flag','Social policy','Race',\n",
    "#             'Racial','Controversial','Profanity','Sexual conduct','Graphic violence','Age-inappropriate',\n",
    "#             'Age-appropriate','Sexual acts','Sexualized','Nudity','LGBTQ','Gender ideology','sex assigned at birth',\n",
    "#             '3101','6312(g)','Implied depictions of sexual acts',\n",
    "#             'Partisan, Political, or Social Policy Advocacy', 'Neutrality']\n",
    "\n",
    "keywords = [\"gender theory\",'holocaust denial','critical theory','at birth','male puberty','Critical race theory','Indoctrination','Social policy',\n",
    "            'Racial','Controversial','Profanity','obscene','Graphic violence','transgender','cisgender',\n",
    "            'Age-appropriate','Gender ideology','sex assigned at birth',\n",
    "            '3101','6312(g)','Implied depictions of sexual acts','social justice',\n",
    "            'Partisan, Political, or Social Policy Advocacy', 'Neutrality']\n",
    "\n",
    "# DataFrame to store the extracted data\n",
    "extracted_data = pd.DataFrame(columns=['School District', 'County', 'URL', 'Text Content', 'Keywords Found', 'Number of Keywords Found'])\n",
    "\n",
    "# Get the current date for file naming\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in a1.iterrows():\n",
    "    district = row['school_dis']\n",
    "    county = row['cty_name']\n",
    "    url = row['base_link']\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_element(By.LINK_TEXT, \"POLICIES\")\n",
    "        driver.execute_script('arguments[0].click()', element)\n",
    "        time.sleep(5)\n",
    "        soup_outer = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        href_elements = soup_outer.findChildren(attrs={'href': '#', 'unique': True})\n",
    "        unique_values = [element.get(\"unique\") for element in href_elements]\n",
    "        time.sleep(2)\n",
    "        for unique_value in unique_values:\n",
    "            new_url = row['prePolicyID'] + unique_value\n",
    "            try:\n",
    "                response = requests.get(new_url)\n",
    "                response.raise_for_status()  # Raise an exception if the response status code is not 200\n",
    "\n",
    "                time.sleep(5)\n",
    "                soup_inner = BeautifulSoup(response.content, \"html.parser\")\n",
    "                container_div = soup_inner.find('div', id='policy-content')\n",
    "                if container_div is not None:\n",
    "                    text_content = container_div.get_text(\" \", strip=True)\n",
    "                    found_keywords = [keyword for keyword in keywords if re.search(r'\\b{}\\b'.format(re.escape(keyword)), text_content, re.IGNORECASE)]\n",
    "                    if found_keywords:\n",
    "                        extracted_data = extracted_data.append({'School District': district,\n",
    "                                                                'County': county,\n",
    "                                                                'URL': new_url,\n",
    "                                                                'Keywords Found': found_keywords,\n",
    "                                                                'Number of Keywords Found': len(found_keywords)}, ignore_index=True)\n",
    "                else:\n",
    "                    print(\"Container div not found for\", new_url)\n",
    "            except Exception as e:\n",
    "                # Handle any exception\n",
    "                print(f\"Exception occurred: {str(e)}. Saving current data to CSV...\")\n",
    "                extracted_data.to_csv('extracted_data_crashedp1.csv', index=False)\n",
    "                print(\"Data saved. Script crashed.\")\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to fetch data from {new_url}: {str(e)}\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                # Handle KeyboardInterrupt (Ctrl + C)\n",
    "                print(\"KeyboardInterrupt detected. Saving current data to CSV...\")\n",
    "                extracted_data.to_csv('extracted_data_interruptedp1.csv', index=False)\n",
    "                print(\"Data saved. Script interrupted.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred for base URL {url}: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "csv_filename = f\"policy1_extracted_data_{current_date}.csv\"\n",
    "extracted_data.to_csv(csv_filename, index=False)\n",
    "print(f\"Extracted data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
