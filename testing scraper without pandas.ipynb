{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ca2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856c13f",
   "metadata": {},
   "source": [
    "with open(\"school_mainPA.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    rows = list(csv_reader)[101:103]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9e60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_csv(\"school_mainPA.csv\")\n",
    "url = a1.base_link.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961b3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the keywords we were looking for\n",
    "keywords = [\"sexuality\", 'at birth', 'male puberty', 'Biological', 'Critical race theory',\n",
    "            'Partisan', 'Political', 'Indoctrination', 'Advocacy', 'Flag', 'Social policy', 'Race',\n",
    "            'Racial', 'Controversial', 'Profanity', 'Sexual conduct', 'Graphic violence', 'Age-inappropriate',\n",
    "            'Age-appropriate', 'Sexual acts', 'Sexualized', 'Nudity', 'LGBTQ', 'Gender ideology', 'sex assigned at birth',\n",
    "            '3101', '6312(g)', 'Implied depictions of sexual acts',\n",
    "            'Partisan, Political, or Social Policy Advocacy', 'Neutrality']\n",
    "\n",
    "# List to store the extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Get the current date for file naming\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d378af25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sexuality',\n",
       " 'at birth',\n",
       " 'male puberty',\n",
       " 'Biological',\n",
       " 'Critical race theory',\n",
       " 'Partisan',\n",
       " 'Political',\n",
       " 'Indoctrination',\n",
       " 'Advocacy',\n",
       " 'Flag',\n",
       " 'Social policy',\n",
       " 'Race',\n",
       " 'Racial',\n",
       " 'Controversial',\n",
       " 'Profanity',\n",
       " 'Sexual conduct',\n",
       " 'Graphic violence',\n",
       " 'Age-inappropriate',\n",
       " 'Age-appropriate',\n",
       " 'Sexual acts',\n",
       " 'Sexualized',\n",
       " 'Nudity',\n",
       " 'LGBTQ',\n",
       " 'Gender ideology',\n",
       " 'sex assigned at birth',\n",
       " '3101',\n",
       " '6312(g)',\n",
       " 'Implied depictions of sexual acts',\n",
       " 'Partisan, Political, or Social Policy Advocacy',\n",
       " 'Neutrality']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba000a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c_ull\\AppData\\Local\\Temp\\ipykernel_6620\\1661116224.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt detected. Continuing to the next URL.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "# Use JavaScript to click the \"POLICIES\" element\n",
    "element = driver.find_element(By.LINK_TEXT, \"POLICIES\")\n",
    "driver.execute_script(\"arguments[0].click();\", element)\n",
    "\n",
    "# Wait for another element to appear (replace with the actual condition)\n",
    "wait = WebDriverWait(driver, timeout=10)\n",
    "element_to_wait_for = wait.until(EC.presence_of_element_located((By.ID, \"ui-id-3\")))\n",
    "soup_outer = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "href_elements = soup_outer.find_all(attrs={'href': '#', 'unique': True})\n",
    "unique_values = [element.get(\"unique\") for element in href_elements]\n",
    "for unique_value in unique_values:\n",
    "    new_url = a1['prePolicyID'].iloc[110] + unique_value\n",
    "    try:\n",
    "        response = requests.get(new_url)\n",
    "        response.raise_for_status()  # Raise an exception if the response status code is not 200\n",
    "        soup_inner = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container_div = soup_inner.find('div', id='policy-content')\n",
    "        if container_div is not None:\n",
    "            text_content = container_div.get_text(\" \", strip=True)\n",
    "            found_keywords = [keyword for keyword in keywords if re.search(r'\\b{}\\b'.format(re.escape(keyword)), text_content, re.IGNORECASE)]\n",
    "            if found_keywords:\n",
    "                for keyword in found_keywords:\n",
    "                    text_content = re.sub(r'\\b({})\\b'.format(re.escape(keyword)), r'<b>\\1</b>', text_content, flags=re.IGNORECASE)\n",
    "                extracted_data.append({'School District': district,\n",
    "                                       'County': county,\n",
    "                                       'URL': new_url,\n",
    "                                       'Text Content': text_content,\n",
    "                                       'Keywords Found': found_keywords,\n",
    "                                       'Number of Keywords Found': len(found_keywords)})\n",
    "        else:\n",
    "            print(\"Container div not found for\", new_url)\n",
    "    except Exception as e:\n",
    "        # Handle any exception. If the error would cause the program to stop running, it will first\n",
    "        # print a message and continue.\n",
    "        print(f\"Exception occurred: {str(e)}. Continuing to the next URL.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # This is in case there's something wrong with the policy urls. It prints a warning and moves on.\n",
    "        print(f\"Failed to fetch data from {new_url}: {str(e)}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Handle KeyboardInterrupt (Ctrl + C). It will print a message and continue.\n",
    "        print(\"KeyboardInterrupt detected. Continuing to the next URL.\")\n",
    "\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f9465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTGMPJ572BA6',\n",
       " 'CTGMD356C1CA',\n",
       " 'CTGMGJ56E40D',\n",
       " 'CBVJU94E7EBE',\n",
       " 'CBVJUB4E7ECD',\n",
       " 'CBVJUD4E7ECF',\n",
       " 'CBVJUF4E7ED2',\n",
       " 'CBVJUH4E7ED5',\n",
       " 'CBVJUK4E7ED8',\n",
       " 'CBVJUM4E7EDB',\n",
       " 'CBVJUP4E7EDF',\n",
       " 'CBVJUR4E7EE3',\n",
       " 'CLP5RY70EADD',\n",
       " 'CDQNS9614947',\n",
       " 'CBAR4A6BA19D',\n",
       " 'CDQNSB614958',\n",
       " 'CDQNSD61495C',\n",
       " 'CDQNST61496B',\n",
       " 'CDQNT9614974',\n",
       " 'CDQNTL61497E',\n",
       " 'CDQNTN614981',\n",
       " 'CDQNTQ614984',\n",
       " 'CDQNTS614988',\n",
       " 'CDQNTU61498A',\n",
       " 'CDQNTW61498D',\n",
       " 'CDQNTY614990',\n",
       " 'CDQNU2614997',\n",
       " 'CDQNU461499B',\n",
       " 'CDQNU66149A2',\n",
       " 'CDQNU86149A6',\n",
       " 'CDQNUA6149AC',\n",
       " 'CDQNUC6149B5',\n",
       " 'CDQNUE6149BA',\n",
       " 'CDQNUG6149BD',\n",
       " 'CDQNUJ6149C3',\n",
       " 'CDQNUL6149C6',\n",
       " 'CDQNUN6149C9',\n",
       " 'CDQNUQ6149CC',\n",
       " 'CDQNUS6149CF',\n",
       " 'CDQNUU6149D1',\n",
       " 'CDQNUY6149D6',\n",
       " 'CCLNLF607238',\n",
       " 'CCQJ3V4B4542',\n",
       " 'CDQNV66149DE',\n",
       " 'CDQNV86149E0',\n",
       " 'CDQNVA6149E2',\n",
       " 'CDQNVC6149E6',\n",
       " 'CDQNVE6149E8',\n",
       " 'CDQNVG6149EC',\n",
       " 'CDQNVL6149F3',\n",
       " 'CPBNR75FFCA5',\n",
       " 'CPBNRL5FFCAC',\n",
       " 'CPJN9U5ECB78',\n",
       " 'CPBNS65FFCB2',\n",
       " 'CDQNVS6149FF',\n",
       " 'CDQNVU614A02',\n",
       " 'CDQNVW614A05',\n",
       " 'CDQNVY614A08',\n",
       " 'CDQNW2614A0B',\n",
       " 'CDQNW4614A0E',\n",
       " 'CDQNW6614A12',\n",
       " 'CDQNW8614A17',\n",
       " 'CDQNWA614A21',\n",
       " 'CLP5RZ70EAE6',\n",
       " 'CFGQ2X673AD1',\n",
       " 'CLP5S670EB0A',\n",
       " 'CFGQ33673AD7',\n",
       " 'CFGQ35673ADB',\n",
       " 'CLP5S770EB12',\n",
       " 'CFGQ39673AE2',\n",
       " 'CFGQ3B673AE4',\n",
       " 'CFGQ3D673AE6',\n",
       " 'CFGQ3F673AE8',\n",
       " 'CFGQ3H673AEA',\n",
       " 'CFGQ3K673AEE',\n",
       " 'CFGQ3M673AF1',\n",
       " 'CFGQ3P673AF3',\n",
       " 'CFGQ3U673AF8',\n",
       " 'CFGQ3Y673AFC',\n",
       " 'CFGQ44673B00',\n",
       " 'CFGQ46673B03',\n",
       " 'CFGQ48673B05',\n",
       " '9P5EWM3C5B98',\n",
       " 'CLP5SC70EB1F',\n",
       " 'CFGQ4E673B0C',\n",
       " 'CFGQ4G673B0F',\n",
       " 'CFGQ4J673B11',\n",
       " 'CFGQ4L673B13',\n",
       " 'CFGQ4N673B16',\n",
       " 'CFGQ4Q673B18',\n",
       " 'CLP5SH70EB32',\n",
       " 'CFGQ4X673B1E',\n",
       " 'CFGQ4Z673B21',\n",
       " 'CFGQ53673B22',\n",
       " 'CFGQ57673B26',\n",
       " 'CFGQ59673B28',\n",
       " 'CFGQ5E673B2C',\n",
       " 'CFGQ5G673B2E',\n",
       " 'CFGQ5J673B30',\n",
       " 'CFGQ5L673B32',\n",
       " 'CFGQ5N673B34',\n",
       " 'CLP5SJ70EB37',\n",
       " 'CFGQ5S673B38',\n",
       " 'CFGQ5U673B39',\n",
       " 'CFGQ5W673B3B',\n",
       " 'CFGQ5Y673B3D',\n",
       " 'CHRJCA4C8156',\n",
       " 'CFGQ64673B41',\n",
       " 'CFGQ66673B43',\n",
       " 'CFGQ6A673B47',\n",
       " 'CFGQ6E673B4A',\n",
       " 'CFGQ6G673B4D',\n",
       " 'CFGQ6M673B50',\n",
       " 'CFGQ6S673B53',\n",
       " 'CLP5SK70EB40',\n",
       " 'CFGQ6W673B57',\n",
       " 'CJ9TB970D1FD',\n",
       " 'CJ9TBB70D20C',\n",
       " 'CJ9TBD70D20F',\n",
       " 'CJ9TBF70D212',\n",
       " 'CJ9TBH70D21B',\n",
       " 'CJ9TBK70D21D',\n",
       " 'CJ9TBM70D21F',\n",
       " 'CJ9TBP70D221',\n",
       " 'CJ9TBR70D223',\n",
       " 'CJ9TBV70D228',\n",
       " 'CJ9TBX70D22B',\n",
       " 'CJ9TBZ70D22E',\n",
       " 'CJ9TC370D230',\n",
       " 'CJ9TC570D234',\n",
       " 'CJ9TC970D239',\n",
       " 'CJ9TCB70D23B',\n",
       " 'CJ9TCD70D23E',\n",
       " 'CJ9TCF70D240',\n",
       " 'CJ9TCH70D242',\n",
       " 'CJ9TCK70D246',\n",
       " 'CJ9TCM70D248',\n",
       " 'CJ9TCP70D24B',\n",
       " 'CJ9TCR70D24D',\n",
       " 'CJ9TCT70D250',\n",
       " 'CJ9TCV70D252',\n",
       " 'CJ9TCX70D25A',\n",
       " 'CJ9TD370D261',\n",
       " 'CJ9TD570D265',\n",
       " 'CJ9TD770D269',\n",
       " 'CJ9TD970D26D',\n",
       " 'CJ9TDB70D271',\n",
       " 'CJ9TDD70D276',\n",
       " 'CJ9TDF70D27A',\n",
       " 'CJ9TDJ70D282',\n",
       " 'CJ9TDU70D285',\n",
       " 'CJ9TEJ70D289',\n",
       " 'CJ9TEL70D293',\n",
       " 'CJ9TEN70D29A',\n",
       " 'CJ9TEQ70D29F',\n",
       " 'CJ9TES70D2A5',\n",
       " 'CJ9TEU70D2AA',\n",
       " 'CJ9TEW70D2AD',\n",
       " 'CJ9TEY70D2B3',\n",
       " 'CJ9TF270D2B6',\n",
       " 'CLQL5P535E62',\n",
       " 'CLQL5R535E69',\n",
       " 'CLQL5T535E6C',\n",
       " 'CLQL5V535E6E',\n",
       " 'CLQL5X535E71',\n",
       " 'CLQL5Z535E73',\n",
       " 'CLQL63535E75',\n",
       " 'CLQL65535E78',\n",
       " 'CLQL67535E7A',\n",
       " 'CLQL69535E7E',\n",
       " 'CLQL6B535E81',\n",
       " 'CLQL6D535E83',\n",
       " 'CLQL6F535E85',\n",
       " 'CLQL6H535E87',\n",
       " 'CLQL6K535E89',\n",
       " 'CLQL6M535E8B',\n",
       " 'CLQL6P535E8E',\n",
       " 'CLQL6R535E90',\n",
       " 'CLQL6T535E92',\n",
       " 'CLQL6V535E94',\n",
       " 'CLQL6X535E97',\n",
       " 'CLQL6Z535E99',\n",
       " 'CLQL73535E9C',\n",
       " 'CLQL75535E9E',\n",
       " 'CQCMRQ5C885A',\n",
       " 'CLQL7Q535EAB',\n",
       " 'CL3RDH6D7CFA',\n",
       " 'CPQLS2563202',\n",
       " 'CPQLS4563209',\n",
       " 'CPQLS856320D',\n",
       " 'CPQLSA56320F',\n",
       " 'CPQLSC563211',\n",
       " 'CPQLSE563213',\n",
       " 'CPQLSG563216',\n",
       " 'CPQLSJ563218',\n",
       " 'CPQLSN56321B',\n",
       " 'CPQLSW563221',\n",
       " 'CPQLSY563223',\n",
       " 'CPQLT4563228',\n",
       " 'CPQLT856322B',\n",
       " 'CPQLTA56322D',\n",
       " 'CPQLTC56322E',\n",
       " '9P5F743D9890',\n",
       " 'CS7K654CA73C',\n",
       " 'CS7K674CA740',\n",
       " 'CS7K694CA744',\n",
       " 'CS7K6B4CA746',\n",
       " 'CS7K6D4CA74C',\n",
       " 'CS7K6F4CA74E',\n",
       " 'CS7K6L4CA753',\n",
       " 'CS7K6N4CA756',\n",
       " 'CS7K6Q4CA759',\n",
       " 'CS7K6S4CA760',\n",
       " 'CS7K6U4CA762',\n",
       " 'CS7K6Z4CA767',\n",
       " 'CS7K734CA76A',\n",
       " 'CS7K794CA772',\n",
       " 'AHYRND6E43EC',\n",
       " 'CS7K7D4CA777',\n",
       " 'CS7K7F4CA77A',\n",
       " 'CS7K7H4CA77C',\n",
       " 'CS7K7K4CA77F',\n",
       " 'CS7K7M4CA782',\n",
       " 'CS7K7S4CA788',\n",
       " 'CS7K7U4CA78B',\n",
       " 'CS7K7W4CA78E',\n",
       " 'CS7K7Y4CA791',\n",
       " 'CS7K824CA794',\n",
       " 'CS7K844CA796',\n",
       " 'CS7K8A4CA79D',\n",
       " 'CS7K8C4CA7A3',\n",
       " '9P5FB23E2D20',\n",
       " 'CS7K8J4CA7AB',\n",
       " 'CTUQX66B5C6B',\n",
       " 'CTUQX86B5C79',\n",
       " '9P5FB93E3484',\n",
       " 'CTUQXC6B5C7F',\n",
       " 'CTUQXK6B5C89',\n",
       " 'CTUQXM6B5C8D',\n",
       " 'CTUQXP6B5C8F',\n",
       " 'CTUQXR6B5C93',\n",
       " 'CTUQXT6B5C96',\n",
       " 'CTUQXV6B5C98',\n",
       " 'CTUQXX6B5C9C',\n",
       " 'CTUQXZ6B5C9E',\n",
       " 'CFGQ77673B5F',\n",
       " 'CHRH6N46DF90',\n",
       " 'CHRH6Z470221',\n",
       " 'CTUQY56B5CA3',\n",
       " 'CTUQY76B5CA6',\n",
       " 'CTUQY96B5CA9',\n",
       " 'CTUQYB6B5CAC',\n",
       " 'CTUQYD6B5CAF']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c283bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    district = row['school_dis']\n",
    "    county = row['cty_name']\n",
    "    url = row['base_link']\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_element(By.LINK_TEXT, \"POLICIES\")\n",
    "        driver.execute_script('arguments[0].click()', element)\n",
    "        time.sleep(5)\n",
    "        soup_outer = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        href_elements = soup_outer.find_all(attrs={'href': '#', 'unique': True})\n",
    "        unique_values = [element.get(\"unique\") for element in href_elements]\n",
    "        time.sleep(2)\n",
    "        for unique_value in unique_values:\n",
    "            new_url = row['prePolicyID'] + unique_value\n",
    "            try:\n",
    "                response = requests.get(new_url)\n",
    "                response.raise_for_status()  # Raise an exception if the response status code is not 200\n",
    "\n",
    "                time.sleep(5)\n",
    "                soup_inner = BeautifulSoup(response.content, \"html.parser\")\n",
    "                container_div = soup_inner.find('div', id='policy-content')\n",
    "                if container_div is not None:\n",
    "                    text_content = container_div.get_text(\" \", strip=True)\n",
    "                    found_keywords = [keyword for keyword in keywords if re.search(r'\\b{}\\b'.format(re.escape(keyword)), text_content, re.IGNORECASE)]\n",
    "                    if found_keywords:\n",
    "                        for keyword in found_keywords:\n",
    "                            text_content = re.sub(r'\\b({})\\b'.format(re.escape(keyword)), r'<b>\\1</b>', text_content, flags=re.IGNORECASE)\n",
    "                        extracted_data.append({'School District': district,\n",
    "                                               'County': county,\n",
    "                                               'URL': new_url,\n",
    "                                               'Text Content': text_content,\n",
    "                                               'Keywords Found': found_keywords,\n",
    "                                               'Number of Keywords Found': len(found_keywords)})\n",
    "                else:\n",
    "                    print(\"Container div not found for\", new_url)\n",
    "            except Exception as e:\n",
    "                # Handle any exception. If the error would cause the program to stop running, it will first\n",
    "                # print a message and continue.\n",
    "                print(f\"Exception occurred: {str(e)}. Continuing to the next URL.\")\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # This is in case there's something wrong with the policy urls. It prints a warning and moves on.\n",
    "                print(f\"Failed to fetch data from {new_url}: {str(e)}\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                # Handle KeyboardInterrupt (Ctrl + C). It will print a message and continue.\n",
    "                print(\"KeyboardInterrupt detected. Continuing to the next URL.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "    except Exception as e:\n",
    "        # If it can't find the \"policies\" tab on the site, it'll print an error message with the URL that didn't work and move on\n",
    "        # to the next URL.\n",
    "        print(f\"Exception occurred for base URL {url}: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "csv_filename = f\"policy3_extracted_data_{current_date}.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = ['School District', 'County', 'URL', 'Text Content', 'Keywords Found', 'Number of Keywords Found']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(extracted_data)\n",
    "\n",
    "print(f\"Extracted data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c198f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca67249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34371074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c_ull\\AppData\\Local\\Temp\\ipykernel_8736\\462522195.py:39: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt detected. Continuing to the next URL.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Read in the main database of boarddocs links. I had to divide the main list into sets of 50.\n",
    "# This scraper will go through 50 districts in about 24 hours. I just copied this code into 9 notebooks and ran each one \n",
    "# at the same time. Just change a1[101:150] to any range you prefer or just cut that line entirely. \n",
    "with open(\"school_mainPA.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    rows = list(csv_reader)[101:105]\n",
    "\n",
    "# These are the keywords we were looking for\n",
    "keywords = [\"sexuality\", 'at birth', 'male puberty', 'Biological', 'Critical race theory',\n",
    "            'Partisan', 'Political', 'Indoctrination', 'Advocacy', 'Flag', 'Social policy', 'Race',\n",
    "            'Racial', 'Controversial', 'Profanity', 'Sexual conduct', 'Graphic violence', 'Age-inappropriate',\n",
    "            'Age-appropriate', 'Sexual acts', 'Sexualized', 'Nudity', 'LGBTQ', 'Gender ideology', 'sex assigned at birth',\n",
    "            '3101', '6312(g)', 'Implied depictions of sexual acts',\n",
    "            'Partisan, Political, or Social Policy Advocacy', 'Neutrality']\n",
    "\n",
    "# List to store the extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Get the current date for file naming\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Loop through each row in the CSV\n",
    "for row in rows:\n",
    "    district = row['school_dis']\n",
    "    county = row['cty_name']\n",
    "    url = row['base_link']\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_element(By.LINK_TEXT, \"POLICIES\")\n",
    "        driver.execute_script('arguments[0].click()', element)\n",
    "        time.sleep(5)\n",
    "        soup_outer = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        href_elements = soup_outer.find_all(attrs={'href': '#', 'unique': True})\n",
    "        unique_values = [element.get(\"unique\") for element in href_elements]\n",
    "        time.sleep(2)\n",
    "        for unique_value in unique_values:\n",
    "            new_url = row['prePolicyID'] + unique_value\n",
    "            try:\n",
    "                response = requests.get(new_url)\n",
    "                response.raise_for_status()  # Raise an exception if the response status code is not 200\n",
    "\n",
    "                time.sleep(5)\n",
    "                soup_inner = BeautifulSoup(response.content, \"html.parser\")\n",
    "                container_div = soup_inner.find('div', id='policy-content')\n",
    "                if container_div is not None:\n",
    "                    text_content = container_div.get_text(\" \", strip=True)\n",
    "                    found_keywords = [keyword for keyword in keywords if re.search(r'\\b{}\\b'.format(re.escape(keyword)), text_content, re.IGNORECASE)]\n",
    "                    if found_keywords:\n",
    "                        for keyword in found_keywords:\n",
    "                            text_content = re.sub(r'\\b({})\\b'.format(re.escape(keyword)), r'<b>\\1</b>', text_content, flags=re.IGNORECASE)\n",
    "                        extracted_data.append({'School District': district,\n",
    "                                               'County': county,\n",
    "                                               'URL': new_url,\n",
    "                                               'Text Content': text_content,\n",
    "                                               'Keywords Found': found_keywords,\n",
    "                                               'Number of Keywords Found': len(found_keywords)})\n",
    "                else:\n",
    "                    print(\"Container div not found for\", new_url)\n",
    "            except Exception as e:\n",
    "                # Handle any exception. If the error would cause the program to stop running, it will first\n",
    "                # print a message and continue.\n",
    "                print(f\"Exception occurred: {str(e)}. Continuing to the next URL.\")\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # This is in case there's something wrong with the policy urls. It prints a warning and moves on.\n",
    "                print(f\"Failed to fetch data from {new_url}: {str(e)}\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                # Handle KeyboardInterrupt (Ctrl + C). It will print a message and continue.\n",
    "                print(\"KeyboardInterrupt detected. Continuing to the next URL.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "    except Exception as e:\n",
    "        # If it can't find the \"policies\" tab on the site, it'll print an error message with the URL that didn't work and move on\n",
    "        # to the next URL.\n",
    "        print(f\"Exception occurred for base URL {url}: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "csv_filename = f\"policy3_extracted_data_{current_date}.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = ['School District', 'County', 'URL', 'Text Content', 'Keywords Found', 'Number of Keywords Found']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(extracted_data)\n",
    "\n",
    "print(f\"Extracted data saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03561816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd02eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Read in the main database of boarddocs links. I had to divide the main list into sets of 50.\n",
    "# This scraper will go through 50 districts in about 24 hours. I just copied this code into 9 notebooks and ran each one \n",
    "# at the same time. Just change a1[101:150] to any range you prefer or just cut that line entirely. \n",
    "a1 = pd.read_csv(\"school_mainPA.csv\")\n",
    "a1 = a1[101:150]\n",
    "#these are the keywords we were looking for\n",
    "keywords = [\"sexuality\",'at birth','male puberty','Biological','Critical race theory',\n",
    "            'Partisan','Political','Indoctrination','Advocacy','Flag','Social policy','Race',\n",
    "            'Racial','Controversial','Profanity','Sexual conduct','Graphic violence','Age-inappropriate',\n",
    "            'Age-appropriate','Sexual acts','Sexualized','Nudity','LGBTQ','Gender ideology','sex assigned at birth',\n",
    "            '3101','6312(g)','Implied depictions of sexual acts',\n",
    "            'Partisan, Political, or Social Policy Advocacy', 'Neutrality']\n",
    "\n",
    "# DataFrame to store the extracted data\n",
    "extracted_data = pd.DataFrame(columns=['School District', 'County', 'URL', 'Text Content', 'Keywords Found', 'Number of Keywords Found'])\n",
    "\n",
    "# Get the current date for file naming\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "# the first try is checking the base policies boarddocs page for each district. It then collects \n",
    "# the unique IDs for each policy and then we use Beautiful Soup to scrape those new urls.\n",
    "# If it finds the keyword in the policy text, it adds it to our database. \n",
    "for index, row in a1.iterrows():\n",
    "    district = row['school_dis']\n",
    "    county = row['cty_name']\n",
    "    url = row['base_link']\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_element(By.LINK_TEXT, \"POLICIES\")\n",
    "        driver.execute_script('arguments[0].click()', element)\n",
    "        time.sleep(5)\n",
    "        soup_outer = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        href_elements = soup_outer.findChildren(attrs={'href': '#', 'unique': True})\n",
    "        unique_values = [element.get(\"unique\") for element in href_elements]\n",
    "        time.sleep(2)\n",
    "        for unique_value in unique_values:\n",
    "            new_url = row['prePolicyID'] + unique_value\n",
    "            try:\n",
    "                response = requests.get(new_url)\n",
    "                response.raise_for_status()  # Raise an exception if the response status code is not 200\n",
    "\n",
    "                time.sleep(5)\n",
    "                soup_inner = BeautifulSoup(response.content, \"html.parser\")\n",
    "                container_div = soup_inner.find('div', id='policy-content')\n",
    "                if container_div is not None:\n",
    "                    text_content = container_div.get_text(\" \", strip=True)\n",
    "                    found_keywords = [keyword for keyword in keywords if re.search(r'\\b{}\\b'.format(re.escape(keyword)), text_content, re.IGNORECASE)]\n",
    "                    if found_keywords:\n",
    "                        for keyword in found_keywords:\n",
    "                            text_content = re.sub(r'\\b({})\\b'.format(re.escape(keyword)), r'<b>\\1</b>', text_content, flags=re.IGNORECASE)\n",
    "                        extracted_data = extracted_data.append({'School District': district,\n",
    "                                                                'County': county,\n",
    "                                                                'URL': new_url,\n",
    "                                                                'Text Content': text_content,\n",
    "                                                                'Keywords Found': found_keywords,\n",
    "                                                                'Number of Keywords Found': len(found_keywords)}, ignore_index=True)\n",
    "                else:\n",
    "                    print(\"Container div not found for\", new_url)\n",
    "            except Exception as e:\n",
    "                # Handle any exception. If the error would cause the program to stop running, it will first\n",
    "                # print a dataframe of what it has so far. \n",
    "                print(f\"Exception occurred: {str(e)}. Saving current data to CSV...\")\n",
    "                extracted_data.to_csv('extracted_data_crashed.csv', index=False)\n",
    "                print(\"Data saved. Script crashed.\")\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                # This is in case there's something wrong with the policy urls. It prints a warning and moves on.\n",
    "                print(f\"Failed to fetch data from {new_url}: {str(e)}\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                # Handle KeyboardInterrupt (Ctrl + C). It will print what it has so far to a datarframe.\n",
    "                print(\"KeyboardInterrupt detected. Saving current data to CSV...\")\n",
    "                extracted_data.to_csv('extracted_data_interrupted.csv', index=False)\n",
    "                print(\"Data saved. Script interrupted.\")\n",
    "\n",
    "            time.sleep(2)\n",
    "# If it can't find the \"policies\" tab on the site, it'll print an error message with the URL that didn't work and move on\n",
    "# to the next URL\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred for base URL {url}: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "csv_filename = f\"policy3_extracted_data_{current_date}.csv\"\n",
    "extracted_data.to_csv(csv_filename, index=False)\n",
    "print(f\"Extracted data saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
